"""
Multi-Head Attention implementation from scratch
from "Attention is All You Need" paper
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math



